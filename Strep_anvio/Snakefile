import glob
import pandas as pd
from snakemake.utils import validate, min_version
import yaml

"""
Author: Vincent F. D. Vendius
Affiliation: Jómsvíkingar
Aim: An'vio pipeline for ancient MAGs on Streptococcus 
Run: snakemake -s Snakefile --use-conda 
"""
min_version("6.0")
configfile:"/projects/sikora/people/xvh856/projects/pipelines/Strep_anvio/config/config.yaml"
workdir:"/projects/sikora/people/xvh856/projects/pipelines/Strep_anvio/"

#file references
TMPPATH=config["tmppath"]
SAMPLESREAD=pd.read_table(config["sampledata"],comment="#",sep="\t", lineterminator="\n"
)
SAMPLETABLE=SAMPLESREAD.set_index(["sampleId"],drop=False)
SAMPLENAMES = SAMPLETABLE.index.unique()
PREFIX=config["prefix"]
ORGANISM=config["organism"]
SUFFIX=config["filesuffix"]
ASSEMBLY_DIR=config["assembly_dir"]
#List of assembly options, non negotiable with current workflow setup
ASSEMBLERS=["Antonio","meta_sensitive","old_megahit"]
#ASSEMBLERS=["Antonio"]

def sample_dir(wildcards):
#   return expand(ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/profilemerged/PROFILE.db",sample=SAMPLENAMES,assembler=ASSEMBLERS,allow_missing=True)
   return expand(ASSEMBLY_DIR + PREFIX + "/{sample}/refinement_instructions.txt",sample=SAMPLENAMES,allow_missing=True)

#looks for profiled databases of samples indicating ready for manual binning
rule all:
    input:
        sample_dir
        #return expand(ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/profilemerged/PROFILE.db",sample=SAMPLENAMES,assembler=ASSEMBLERS,allow_missing=True)
                                            
#use megahit to assembly the raw reads into contigs using de brujin graph architecture
#first attempt this using Antonios recommended parameters
rule read_assembly_antonio:
    input:
        sample_files=(
            lambda wildcards: SAMPLETABLE.collapsed[wildcards.sample]
        ),
    output:
        ASSEMBLY_DIR + PREFIX + "/{sample}/Antonio/final.contigs.fa",
    conda:
        "envs/newmegahit_environment.yaml"
    params:
        outdir=ASSEMBLY_DIR + PREFIX + "/{sample}/Antonio",
        contiglen="1000",
        antonio_settings=config["assembly_params_antonio"],
    threads: config["assembly_threads"]
    shell:
        """
        rm -rf {params.outdir}
        samples=$(echo {input.sample_files} | sed 's/ /,/g')
        megahit -r $samples -o {params.outdir} --min-contig-len {params.contiglen} -t {threads} --tmp-dir {TMPPATH} {params.antonio_settings}
        """
##repeat the same with just meta sensitive parameters for comparison
rule read_assembly_meta_sensitive:
    input:
        sample_files=(
            lambda wildcards: SAMPLETABLE.collapsed[wildcards.sample]
        ),
    output:
        ASSEMBLY_DIR + PREFIX + "/{sample}/meta_sensitive/final.contigs.fa",
    conda:
        "envs/newmegahit_environment.yaml"
    params:
        outdir=ASSEMBLY_DIR + PREFIX + "/{sample}/meta_sensitive",
        contiglen="1000",
        meta_sensitive=config["assembly_params_meta_sensitive"],
    threads: config["assembly_threads"]
    shell:
        """
        rm -rf {params.outdir}
        samples=$(echo {input.sample_files} | sed 's/ /,/g')
        megahit -r $samples -o {params.outdir} --min-contig-len {params.contiglen} -t {threads} --tmp-dir {TMPPATH} {params.meta_sensitive}
        """
#and finally to be sure, load up a version of megahit that worked weirdly well on NEO137
rule read_assembly_old:
    input:
        sample_files=(
            lambda wildcards: SAMPLETABLE.collapsed[wildcards.sample]
        ),
    output:
        ASSEMBLY_DIR + PREFIX + "/{sample}/old_megahit/final.contigs.fa",
    conda:
        "envs/oldmegahit_environment.yaml"
    params:
        outdir=ASSEMBLY_DIR + PREFIX + "/{sample}/old_megahit",
        contiglen="1000",
        meta_sensitive=config["assembly_params_meta_sensitive"],
    threads: config["assembly_threads"]
    shell:
        """
        rm -rf {params.outdir}
        samples=$(echo {input.sample_files} | sed 's/ /,/g')
        megahit -r $samples -o {params.outdir} --min-contig-len {params.contiglen} -t {threads} --tmp-dir {TMPPATH} {params.meta_sensitive}
        """
        
#Safety change of megahit contig names to be compatible with anvio
#this rule is to filter out small contigs aswell, which might not be so informative
rule filter_contigs:
    input:
        ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/final.contigs.fa"
    output:
        qcfa=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/FXD.contigs.fa",
    params:
        minlen=config["minlen_db"]
    shell:
        """
        anvi-script-reformat-fasta {input} --prefix {PREFIX} --min-len {params.minlen} --simplify-names -o {output.qcfa}
        """

#generate a contig database of metadata, which i pad with contig profile information
rule contig_db:
    input:
        ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/FXD.contigs.fa"
    output:
        cdb=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/contigs.db"
    params:
        tmppath=TMPPATH + "/{sample}/{assembler}"
    threads: config["contigdb_threads"]
    shell:
        """
        if [ -d {params.tmppath} ]; then
            rm -rf {params.tmppath}
            mkdir -p {params.tmppath}
        else
            mkdir -p {params.tmppath}
        fi
        anvi-gen-contigs-database -f {input} -o {output.cdb} --num-threads {threads} -n {wildcards.sample}
        anvi-run-hmms -c {output.cdb} --num-threads {threads}
        anvi-run-scg-taxonomy -c {output.cdb} --num-threads {threads}
        anvi-run-ncbi-cogs -c {output.cdb} --temporary-dir-path {params.tmppath} --num-threads {threads}
        """

#once a contig database is completed it is time to align the samples against the contigs
#we will use bowtie2, which requires us to index the contigs first
rule index:
    input:
        qcfa=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/FXD.contigs.fa",
    output:
        multiext(ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/ref_index/" + PREFIX,".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"),
    params:
        bowtie_index_dir=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/ref_index",
    shell:
        """
        if [ -d {params.bowtie_index_dir} ]; then
            rm -rf {params.bowtie_index_dir}
            mkdir -p {params.bowtie_index_dir}
        else
            mkdir -p {params.bowtie_index_dir}
        fi
        bowtie2-build  --threads {threads} {input.qcfa} {params.bowtie_index_dir}/{PREFIX}
        """

#Alignment of sample reads against sample set contig database
#and then profiling
#i set up a bash script that identifies if more than one library is associated with each sample name
#the rule then runs mapping on each library before merging the profile databases
rule mapping:
    input:
        multiext(ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/ref_index/" + PREFIX,".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"),
        fq_file=(
            lambda wildcards: SAMPLETABLE.collapsed[wildcards.sample]
            #lambda wildcards: SAMPLETABLE.loc[SAMPLETABLE['library'] == wildcards.library].collapsed[wildcards.sample]
        ),
        cdb=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/contigs.db",
    output:
        profile=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/profilemerged/PROFILE.db"
    params:
        profile=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/profile",
        profilemerged=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/profilemerged",
        bowtie_index_dir=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/ref_index",
        bowtie_params=config["bowtie_params"],
        bam=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/map_",
        #suffix=".mutans.x_human.fq.gz",
        suffix=".trim.fq.gz",
    threads:config["profile_threads"] 
    #conda:
    #    "envs/mapping.yaml"
    shell:
        """
        path_array=(${input.fq_file})
        if [[ ${{#path_array[@]}} -ge 2 ]]; then
            echo hello
            for library in {input.fq_file}
            do
                LIBID=$(echo $library | sed 's/.*\///'g |sed 's/{params.suffix}//g')
                echo $LIBID
                bowtie2 {params.bowtie_params} --threads {threads} -x {params.bowtie_index_dir}/{PREFIX} \
                -U $library \
                 | samtools sort -@ {threads} -T /dev/shm/ | samtools view -F 4 -bh > {params.bam}$LIBID.bam
                samtools index {params.bam}$LIBID.bam -@ {threads} 
                rm -rf {params.profile}$LIBID
                anvi-profile -c {input.cdb} -i {params.bam}$LIBID.bam --num-threads {threads} -o {params.profile}$LIBID --min-coverage-for-variability 5 --cluster-contigs
            done
            rm -rf {params.profilemerged}
            anvi-merge {params.profile}*/PROFILE.db -o {params.profilemerged} -c {input.cdb}
        else
            bowtie2 {params.bowtie_params} --threads {threads} -x {params.bowtie_index_dir}/{PREFIX} \
            -U {input.fq_file} \
             | samtools sort -@ {threads} -m 1G -T /dev/shm/ | samtools view -F 4 -bh > {params.bam}.bam
            samtools index {params.bam}.bam -@ {threads}
            rm -rf {params.profilemerged}
            anvi-profile -c {input.cdb} -i {params.bam}.bam --num-threads {threads} -o {params.profilemerged} --min-coverage-for-variability 5 --cluster-contigs
        fi
        """

#do automatic binning using concoct and metabat on all assembler/sample combinations
#do not multithread this one, it will break down 
#if a sample has a merged profile database from multiple libraries, i can use anvios built in binning function
#otherwise for "single sample" assembly, i have to access the software directly
rule binning:
    input:
        qcfa=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/FXD.contigs.fa",
        cdb=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/contigs.db",
        fq_file=(
            lambda wildcards: SAMPLETABLE.collapsed[wildcards.sample]
            #lambda wildcards: SAMPLETABLE.loc[SAMPLETABLE['library'] == wildcards.library].collapsed[wildcards.sample]
        ),
        profile=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/profilemerged/PROFILE.db"
    output:
        concoct_summary=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/concoct-summary/bins_summary.txt",
        metabat_summary=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/metabat-summary/bins_summary.txt",
    params:
        concoct_bed=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/contigs_10K.bed",
        concoct_fa=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/contigs_10K.fa",
        concoct_table=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/coverage_table.tsv",
        concoct_dir=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/concoct_output/",
        concoct_collection=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/concoct_collection.tsv",
        concoct_out_dir=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/concoct-summary",
        metabat_collection=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/metabat2_collection.tsv", 
        metabat_out_dir=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/metabat-summary",
        metabat_dat=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/metabat",
        bam=ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/map_",
    threads: config["binning_threads"]
    shell:
        """
        path_array=(${input.fq_file})
        if [[ ${{#path_array[@]}} -ge 2 ]]; then
            echo first concoct
            anvi-cluster-contigs -c {input.cdb} -p {input.profile} -C CONCOCT --driver CONCOCT --just-do-it --num-threads {threads}
            echo and now metabat
            anvi-cluster-contigs -c {input.cdb} -p {input.profile} -C METABAT --driver METABAT2 --just-do-it --num-threads {threads} 
        else
            echo first concoct
            cut_up_fasta.py {input.qcfa} -c 10000 -o 0 --merge_last -b {params.concoct_bed} > {params.concoct_fa}
            concoct_coverage_table.py {params.concoct_bed} {params.bam}.bam > {params.concoct_table}
            concoct --composition_file {params.concoct_fa} --coverage_file {params.concoct_table} -b {params.concoct_dir} 
            merge_cutup_clustering.py {params.concoct_dir}clustering_gt1000.csv > {params.concoct_dir}clustering_merged.csv
            sed -i s/,/\\\tbin_/g {params.concoct_dir}clustering_merged.csv
            tail {params.concoct_dir}clustering_merged.csv -n +2 > {params.concoct_collection}
            rm {params.concoct_dir}* {params.concoct_bed} {params.concoct_table} {params.concoct_fa}
            rmdir {params.concoct_dir}
            anvi-import-collection {params.concoct_collection} -c {input.cdb} -p {input.profile} -C CONCOCT --contigs-mode
            echo and now metabat
            jgi_summarize_bam_contig_depths --outputDepth depth.txt {params.bam}.bam
            metabat2 -i {input.qcfa} -a depth.txt -o {params.metabat_dat}/bin -l
            rename -v '.' _ {params.metabat_dat}/*
            cd {params.metabat_dat}
            awk '{{print $0 \"\\t\" FILENAME;}}' * > {params.metabat_collection}
            cd ..
            rm {params.metabat_dat}/*
            rmdir {params.metabat_dat}
            anvi-import-collection {params.metabat_collection} -c {input.cdb} -p {input.profile} -C METABAT --contigs-mode
        fi
        anvi-summarize -c {input.cdb} \
           -p {input.profile} \
           -C CONCOCT \
           -o {params.concoct_out_dir} --force-overwrite
        anvi-summarize -c {input.cdb} \
           -p {input.profile} \
           -C METABAT \
           -o {params.metabat_out_dir} --force-overwrite
        """

#This rule is supposed to go through the anvio summaries, which are produced in the binning rule for each assembler/sample combination
#it will then search for a bin corresponding to the organism being assembled and see which binner/assembler combination created the most viable collection of contigs
#this is currently only evaluated on estimated completion, however it should be tuned to also include information such as genome length and gc content, aswell as redundancy
#the output of the rule is the file path to the contigs for the best binner/assembly combination

#new directive as of 15/07/23. Choose Antonio, choose concoct!
rule binning_summary:
    input:
        expand(ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/{summary}/bins_summary.txt",assembler=ASSEMBLERS,summary=["metabat-summary","concoct-summary"],allow_missing=True)
        #binning_files
    output:
        bin_sum=ASSEMBLY_DIR + PREFIX + "/{sample}/best_binning.txt"    
    params:
        bestbin=ASSEMBLY_DIR + PREFIX + "/{sample}/Antonio/concoct-summary/bins_summary.txt"
    shell:
        "grep \"{ORGANISM}\" {input} | grep \"Antonio\" | grep \"concoct\" | sed 's/:/\\t/g' | sort -k6 -nr | head -n 1 | cut -f1,2 |"
        " awk '$3=$2' | sed 's/bins_summary.txt /bin_by_bin\\//g' | sed 's/ /\\//g' | sed 's/.*/&-contigs.fa/g' > {output}"

        #"grep \"{ORGANISM}\" {params.bestbin} | sed 's/:/\\t/g' | cut -f1,2 |"
#this rule takes in the file path to the contigs for the organism and goes through the standard procedure of creating an anvio contigs database.
#this database is gilded with as much information as anvio can find for pangenomics
rule assembly_db: 
    input:
        bin_sum=ASSEMBLY_DIR + PREFIX + "/{sample}/best_binning.txt"    
    output:
        assembly_cdb=ASSEMBLY_DIR + PREFIX + "/best_binning_contigs/{sample}_contigs.db",    
    threads: config["contigdb_threads"]
    shell:
        """
        FASTA=$(cat {input.bin_sum})
        echo $FASTA
        anvi-gen-contigs-database -f $FASTA -o {output.assembly_cdb} --num-threads {threads} -n {wildcards.sample}
        anvi-run-hmms -c {output.assembly_cdb} --num-threads {threads}
        anvi-run-scg-taxonomy -c {output.assembly_cdb} --num-threads {threads}
        anvi-run-ncbi-cogs -c {output.assembly_cdb} --temporary-dir-path {TMPPATH} --num-threads {threads}
        anvi-run-pfams -c {output.assembly_cdb} --num-threads {threads}
        anvi-run-kegg-kofams -c {output.assembly_cdb} -T {threads}
        """

#This is a file with columns: name of sample and where its contigs database is located for pangenomics
rule external_genomes:
    input:
        expand(ASSEMBLY_DIR + PREFIX + "/best_binning_contigs/{sample}_contigs.db",sample=SAMPLENAMES,allow_missing=True)    
    output:
        exgenomes=ASSEMBLY_DIR + PREFIX + "/external-genomes.txt"
    params:
        contigpath=ASSEMBLY_DIR + PREFIX + "/best_binning_contigs"
    shell:
        "anvi-script-gen-genomes-file --input-dir {params.contigpath} --output-file {output}"

#We generate a genomes storage for the pangenomic analysis
rule genome_storage:
     input:
        exgenomes=ASSEMBLY_DIR + PREFIX + "/external-genomes.txt"
     output:
        genstor=ASSEMBLY_DIR + PREFIX + "/PAN_ex-GENOMES.db"
     shell:
        "anvi-gen-genomes-storage -e {input.exgenomes} -o {output.genstor}"

#Computation of pan genome from the external storage of genomes
rule pangenome:
    input:
       genstor=ASSEMBLY_DIR + PREFIX + "/PAN_ex-GENOMES.db",
    output:
       pangen=ASSEMBLY_DIR + PREFIX + "/pangenome/" + PREFIX + "-PAN.db"
    params:
       pangen=ASSEMBLY_DIR + PREFIX + "/pangenome"
    threads: config["assembly_threads"]
    shell:
       """
       rm -rf {params.pangen}
       anvi-pan-genome -g {input} --use-ncbi-blast --minbit 0.5 --mcl-inflation 10 --project-name {PREFIX} --num-threads {threads} -o {params.pangen} --force-overwrite
       """


#Gene clusters that are only present in one MAG are identified and their sequences are extracted
rule singleton_cluster:
    input:
       genstor=ASSEMBLY_DIR + PREFIX + "/PAN_ex-GENOMES.db",
       pangen=ASSEMBLY_DIR + PREFIX + "/pangenome/" + PREFIX + "-PAN.db",
    output:
       singletons=ASSEMBLY_DIR + PREFIX + "/singleton_gene_clusters.fa"
    shell:
        """
        anvi-get-sequences-for-gene-clusters -p {input.pangen} \
                                     -g {input.genstor} \
                                     --report-DNA-sequences --force-overwrite \
                                     --max-num-genomes-gene-cluster-occurs 1 \
                                     --output-file {output.singletons}
        """


#sort bam files and calculate damage on each contig         
rule metadamage:
    input:
        bin_sum=ASSEMBLY_DIR + PREFIX + "/{sample}/best_binning.txt",
        fq_file=(
            lambda wildcards: SAMPLETABLE.collapsed[wildcards.sample]
            #lambda wildcards: SAMPLETABLE.loc[SAMPLETABLE['library'] == wildcards.library].collapsed[wildcards.sample]
        ),
        profile=expand(ASSEMBLY_DIR + PREFIX + "/{sample}/{assembler}/profilemerged/PROFILE.db",assembler=ASSEMBLERS,allow_missing=True),
    output:    
        dmg_contigs=ASSEMBLY_DIR + PREFIX + "/{sample}/metadmg/damage_contigs.tsv",
        dmg_contigs_corrected=ASSEMBLY_DIR + PREFIX + "/{sample}/metadmg/damage_contigs_corrected.tsv",
    conda:
        "envs/metadamage.yaml"
    params:
        metadmg_contig_info=ASSEMBLY_DIR + PREFIX + "/{sample}/metadmg/metadamage.csv",
        dir=ASSEMBLY_DIR + PREFIX + "/{sample}",
        config_file=ASSEMBLY_DIR + PREFIX + "/{sample}/metadmg/metaDMG_config.yaml",
        metadmg_out_dir=ASSEMBLY_DIR + PREFIX + "/{sample}/metadmg/metaDMG_output",
        metadmg_dir=ASSEMBLY_DIR + PREFIX + "/{sample}/metadmg",
        names="/projects/sikora/people/xvh856/bin/ncbi_tax_dmp/names.dmp",
        nodes="/projects/sikora/people/xvh856/bin/ncbi_tax_dmp/nodes.dmp",
        acc2tax="/projects/sikora/people/xvh856/bin/ncbi_tax_dmp/nucl_gb.accession2taxid",
        metaDMG_cpp="/projects/sikora/people/xvh856/bin/metaDMG-cpp/metaDMG-cpp",
    threads: config["metadmg_threads"]
    shell:
        """
        BESTASSEMBLY=$(cat {input.bin_sum} | sed 's/.*{wildcards.sample}\\///g' | sed 's/\/.*//g')
        FASTA=$(cat {input.bin_sum})
        echo $FASTA $BESTASSEMBLY
        path_array=(${input.fq_file})
        if [[ ${{#path_array[@]}} -ge 2 ]]; then
            for library in {input.fq_file}
            do
               LIBID=$(echo $library | sed 's/.*\///'g |sed 's/.mutans.x_human.fq.gz//g')
               samtools sort -@ {threads} -n -m 1G -T /dev/shm/ {params.dir}/$BESTASSEMBLY/map_$LIBID.bam > {params.dir}/$BESTASSEMBLY/map_$LIBID.sorted.bam
            done
        else
            samtools sort -@ {threads} -n -m 1G -T /dev/shm/ {params.dir}/$BESTASSEMBLY/map_.bam > {params.dir}/$BESTASSEMBLY/map_.sorted.bam
        fi
        metaDMG config {params.dir}/$BESTASSEMBLY/*sorted.bam --metaDMG-cpp {params.metaDMG_cpp} \
               --overwrite \
               --config-file {params.config_file} \
               --custom-database \
               --weight-type 1 \
               --names {params.names} \
               --nodes {params.nodes} \
               --acc2tax {params.acc2tax} \
               --parallel-samples 1 \
               --cores-per-sample {threads} \
               --output-dir {params.metadmg_out_dir} \
               --max-position 35  \
               --lca-rank '' \
               --min-similarity-score 0.90 \
               --damage-mode local --bayesian 
        metaDMG compute {params.config_file}
        metaDMG convert --add-fit-predictions --output {params.metadmg_contig_info} --results {params.metadmg_out_dir}/results
        cut {params.metadmg_contig_info} -f2,3,4 -d "," | tail -n +2 | sed 's/,/\\t/g' > {output.dmg_contigs}
        python3 scr/damage_correction.py {output.dmg_contigs} {output.dmg_contigs_corrected} 
        """

        
#add metadata to contigs  
rule singleton_contigs:
    input:
        bin_sum=ASSEMBLY_DIR + PREFIX + "/{sample}/best_binning.txt",
        dmg_contigs_corrected=ASSEMBLY_DIR + PREFIX + "/{sample}/metadmg/damage_contigs_corrected.tsv",
        singletons=ASSEMBLY_DIR + PREFIX + "/singleton_gene_clusters.fa",
    output:
        singleton_contigs=ASSEMBLY_DIR + PREFIX + "/{sample}/singleton_contigs.tsv",
        refine_instructions=ASSEMBLY_DIR + PREFIX + "/{sample}/refinement_instructions.txt",
    params:
        dir=ASSEMBLY_DIR + PREFIX + "/{sample}"
    shell:
        """
        BESTASSEMBLY=$(cat {input.bin_sum} | sed 's/.*{wildcards.sample}\\///g' | sed 's/\/.*//g')
        FASTA=$(cat {input.bin_sum})
        echo $FASTA $BESTASSEMBLY
        python3 scr/linkingscript.py $FASTA {input.singletons} {wildcards.sample} > {output.singleton_contigs} 
        anvi-import-misc-data {output.singleton_contigs} -c {params.dir}/$BESTASSEMBLY/contigs.db \
        -p {params.dir}/$BESTASSEMBLY/profilemerged/PROFILE.db -t items --contigs-mode --just-do-it
        anvi-import-misc-data {input.dmg_contigs_corrected} -c {params.dir}/$BESTASSEMBLY/contigs.db \
        -p {params.dir}/$BESTASSEMBLY/profilemerged/PROFILE.db -t items --contigs-mode --just-do-it
        echo anvi-refine -p {params.dir}/$BESTASSEMBLY/profilemerged/PROFILE.db -c {params.dir}/$BESTASSEMBLY/contigs.db \
        -C INBINNINGSUMMARY -b INBINNINGSUMMARY --server-only > {output.refine_instructions}
        """




#now you shall manually refine!


#zgrep -P "\t1000289607\t" /willerslev/users-shared/science-snm-willerslev-dsw670/projects/diseases/results/pathopipe/classify/Yana.diseases.krakenuniq.class.tsv.gz | cut -f2 > Yana_human_reads
#seqkit grep -f Yana_human_reads -v /willerslev/datasets/WABI_pipeline/fqs/yana/Yana_old.MA845_L1.trim.fq.gz -o Yana_no_human.trim.fq.gz
#include: "rules/generate-contigs.smk"
